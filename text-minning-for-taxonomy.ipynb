{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Text-mining-for-taxonomy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import operator\n",
    "from typing import Annotated, List, Optional, TypedDict\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(\"tnt-llm\")\n",
    "\n",
    "class Doc(TypedDict):\n",
    "    id: str\n",
    "    content: str\n",
    "    summary: Optional[str]\n",
    "    explanation: Optional[str]\n",
    "    category: Optional[str]\n",
    "\n",
    "class TaxonomyGenerationState(TypedDict):\n",
    "    # The raw docs; we inject summaries within them in the first step\n",
    "    documents: List[Doc]\n",
    "    # Indices to be concise\n",
    "    minibatches: List[List[int]]\n",
    "    # Candidate Taxonomes (full trajectory)\n",
    "    clusters: Annotated[List[List[dict]], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize docs\n",
    "\n",
    "Chat logs can get quite long. Our taxonomy generation step needs to see large, diverse minibatches to be able to adequately capture the distribution of categories. To ensure they can all fit efficiently into the context window, we first summarize each chat log. Downstream steps will use these summaries instead of the raw doc content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda, RunnablePassthrough\n",
    "\n",
    "summary_prompt = hub.pull(\"wfh/tnt-llm-summary-generation\").partial(\n",
    "    summary_prompt=20, explanation_length=30\n",
    ")\n",
    "\n",
    "def parse_summary(xml_string: str) -> dict:\n",
    "    summary_pattern = r\"<summary>(.*?)</summary>\"\n",
    "    explanation_pattern = r\"<explanation>(.*?)</explanation>\"\n",
    "\n",
    "    summary_match = re.search(summary_pattern, xml_string, re.DOTALL)\n",
    "    explanation_match = re.search(explanation_pattern, xml_string, re.DOTALL)\n",
    "\n",
    "    summary = summary_match.group(1).strip() if summary_match else \"\"\n",
    "    explanation = explanation = explanation_match.group(1).strip() if explanation_match else \"\"\n",
    "\n",
    "    return {\"summary\": summary, \"explanation\": explanation}\n",
    "\n",
    "summary_llm_chain = (\n",
    "    summary_prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()\n",
    "    # Customise the tracking name for easier organization\n",
    ").with_config(run_name=\"GenerateSummary\")\n",
    "summary_chain = summary_llm_chain | parse_summary\n",
    "\n",
    "# Now combine as a \"map\" operation in a map-reduce chain \n",
    "# Input: state\n",
    "# Output: state U summaries\n",
    "# Processes docs in parallel\n",
    "def get_content(state: TaxonomyGenerationState):\n",
    "    docs = state['documents']\n",
    "    return [{\"content\": doc['content']} for doc in docs]\n",
    "\n",
    "map_step = RunnablePassthrough.assign(\n",
    "    summaries=get_content\n",
    "    # This effectively creates a \"map\" operation\n",
    "    # Note you can make this more robust by handling individual errors\n",
    "    | RunnableLambda(func=summary_chain.batch, afunc=summary_chain.abatch)\n",
    ")\n",
    "\n",
    "def reduce_summaries(combined: dict) -> TaxonomyGenerationState:\n",
    "    summaries = combined['summaries']\n",
    "    documents = combined['documents']\n",
    "    return {\n",
    "        \"documents\": [\n",
    "            {\n",
    "                \"id\": doc['id'],\n",
    "                \"content\": doc['content'],\n",
    "                \"summary\": summ_info['summary'],\n",
    "                \"explanation\": summ_info['explanation'],\n",
    "            }\n",
    "            for doc, summ_info in zip(documents, summaries)\n",
    "        ]\n",
    "    }\n",
    "map_reduce_chain = map_step | reduce_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into Minibatches\n",
    "\n",
    "Each minibatch contains a random sample of docs. This lets the flow identify inadequacies in the current taxonomy using new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_minibatches(state: TaxonomyGenerationState, config: RunnableConfig):\n",
    "    batch_size = config['configurable'].get(\"batch_size\", 200)\n",
    "    original = state['documents']\n",
    "    indices = list(range(len(original)))\n",
    "    random.shuffle(indices)\n",
    "    if len(indices) < batch_size:\n",
    "        # Don't pad needlessly if we can't fill a single batch\n",
    "        return [indices]\n",
    "    \n",
    "    num_full_batches = len(indices) // batch_size\n",
    "\n",
    "    batches = [\n",
    "        indices[i * batch_size: (i+1) * batch_size] for i in range(num_full_batches)\n",
    "    ]\n",
    "    leftovers = len(indices) % batch_size\n",
    "    if leftovers:\n",
    "        last_batch = indices[num_full_batches * batch_size :]\n",
    "        elements_to_add = batch_size - leftovers\n",
    "        last_batch += random.sample(indices, elements_to_add)\n",
    "        batches.append(last_batch)\n",
    "\n",
    "    return {\n",
    "        \"minibatches\": batches,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxonomy Generation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "def parse_taxa(output_text: str) -> Dict:\n",
    "    \"\"\"Extract the taxonomy from the generated output.\"\"\"\n",
    "    cluster_matches = re.findall(\n",
    "        r\"\\s*<id>(.*?)</id>\\s*<name>(.*?)</name>\\s*<description>(.*?)</description>\\s*\",\n",
    "        output_text,\n",
    "        re.DOTALL,\n",
    "    )\n",
    "    clusters = [\n",
    "        {'id': id.strip(), \"name\": name.strip(), \"description\": description.strip()}\n",
    "        for id, name, description in cluster_matches\n",
    "    ]\n",
    "    # We don't parse the explanation since it isn't used downstream\n",
    "    return {'clusters': clusters}\n",
    "\n",
    "def format_docs(docs: List[Doc]) -> str:\n",
    "    xml_table = \"<conversation>\\n\"\n",
    "    for doc in docs:\n",
    "        xml_table += f'<conv_summ id={doc[\"id\"]}>{doc[\"summary\"]}</conv_summ>\\n'\n",
    "    xml_table +=\"</conversation>\"\n",
    "    return xml_table\n",
    "\n",
    "def format_taxonomy(clusters):\n",
    "    xml = \"<cluster_table>\\n\"\n",
    "    for label in clusters:\n",
    "        xml += \"  <cluster>\\n\"\n",
    "        xml += f'    <id>{label[\"id\"]}</id>\\n'\n",
    "        xml += f'    <name>{label[\"name\"]}</name>\\n'\n",
    "        xml += f'    <description>{label[\"description\"]}</description>\\n'\n",
    "        xml += \"  </cluster>\\n\"\n",
    "    xml += \"</cluster_table>\"\n",
    "    return xml\n",
    "\n",
    "def invoke_taxonomy_chain(\n",
    "        chain: Runnable,\n",
    "        state: TaxonomyGenerationState,\n",
    "        config: RunnableConfig,\n",
    "        mb_indices: List[int],\n",
    ") -> TaxonomyGenerationState:\n",
    "    configurable = config['configurable']\n",
    "    docs = state['documents']\n",
    "    minibatch = [docs[idx] for idx in mb_indices]\n",
    "    data_table_xml = format_docs(minibatch)\n",
    "\n",
    "    previous_taxonomy = state['clusters'][-1] if state['clusters'] else []\n",
    "    cluster_table_xml = format_taxonomy(previous_taxonomy)\n",
    "\n",
    "    updated_taxonomy = chain.invoke(\n",
    "        {\"data_xml\": data_table_xml,\n",
    "         \"use_case\": configurable['use_case'],\n",
    "         \"cluster_table_xml\": cluster_table_xml,\n",
    "         \"suggestion_length\": configurable.get(\"suggestion_length\", 30),\n",
    "         \"cluster_name_length\": configurable.get(\"cluster_name_length\", 10),\n",
    "         \"cluster_description_length\": configurable.get(\"cluster_description_length\", 30),\n",
    "         \"explanation_length\": configurable.get(\"explanation_length\", 20),\n",
    "         \"max_num_clusters\": configurable.get(\"max_num_clusters\", 25)\n",
    "         }\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"clusters\": [updated_taxonomy['clusters']],\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate initial taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain_core/utils/utils.py:234: UserWarning: WARNING! max_tokens_to_sample is not default parameter.\n",
      "                max_tokens_to_sample was transferred to model_kwargs.\n",
      "                Please confirm that max_tokens_to_sample is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We will share an LLM for each step of the generate -> update -> review cycle\n",
    "taxonomy_generation_llm = ChatOpenAI(model=\"gpt-4o\", max_tokens_to_sample=2000)\n",
    "\n",
    "# Initial generation\n",
    "taxonomy_generation_prompt = hub.pull(\"wfh/tnt-llm-taxonomy-generation\").partial(\n",
    "    use_case='Generate the taxonomy that can be used to label the user intent in the conversation.',\n",
    ")\n",
    "\n",
    "taxa_gen_llm_chain = (\n",
    "    taxonomy_generation_prompt | taxonomy_generation_llm | StrOutputParser()\n",
    ").with_config(run_name='GenerateTaxonomy')\n",
    "\n",
    "generate_taxonomy_chain = taxa_gen_llm_chain | parse_taxa\n",
    "\n",
    "def generate_taxonomy(\n",
    "        state: TaxonomyGenerationState, config: RunnableConfig\n",
    ") -> TaxonomyGenerationState:\n",
    "    return invoke_taxonomy_chain(\n",
    "        generate_taxonomy_chain, state, config=state['minibatches'][0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_update_prompt = hub.pull(\"wfh/tnt-llm-taxonomy-update\")\n",
    "\n",
    "taxa_update_llm_chain = (\n",
    "    taxonomy_update_prompt | taxonomy_generation_llm | StrOutputParser()\n",
    ").with_config(run_name='UpdateTaxonomy')\n",
    "\n",
    "update_taxonomy_chain = taxa_update_llm_chain | parse_taxa\n",
    "\n",
    "def update_taxonomy(\n",
    "        state: TaxonomyGenerationState, config: RunnableConfig\n",
    ") -> TaxonomyGenerationState:\n",
    "    which_mb = len(state['clusters']) % len(state['minibatches'])\n",
    "    return invoke_taxonomy_chain(\n",
    "        update_taxonomy_chain, state, config, state['minibatches'][which_mb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_review_prompt = hub.pull(\"wfh/tnt-llm-taxonomy-review\")\n",
    "taxa_review_llm_chain = (\n",
    "    taxonomy_review_prompt | taxonomy_generation_llm | StrOutputParser()\n",
    ").with_config(run_name='ReviewTaxonomy')\n",
    "\n",
    "review_taxonomy_chain = taxa_review_llm_chain | parse_taxa\n",
    "\n",
    "def review_taxonomy(\n",
    "        state: TaxonomyGenerationState, config: RunnableConfig\n",
    ") -> TaxonomyGenerationState:\n",
    "    batch_size = config['configurable'].get(\"batch_size\", 200)\n",
    "    original = state['documents']\n",
    "    indices = list(range(len(original)))\n",
    "    random.shuffle(indices)\n",
    "    return invoke_taxonomy_chain(\n",
    "        review_taxonomy_chain, state, config, indices[:batch_size]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "graph = StateGraph(TaxonomyGenerationState)\n",
    "graph.add_node(\"summarize\", map_reduce_chain)\n",
    "graph.add_node(\"get_minibatches\", get_minibatches)\n",
    "graph.add_node(\"generate_taxonomy\", generate_taxonomy)\n",
    "graph.add_node(\"update_taxonomy\", update_taxonomy)\n",
    "graph.add_node(\"review_taxonomy\", review_taxonomy)\n",
    "\n",
    "graph.add_edge('summarize', 'get_minibatches')\n",
    "graph.add_edge(\"get_minibatches\", \"generate_taxonomy\")\n",
    "graph.add_edge(\"generate_taxonomy\", 'update_taxonomy')\n",
    "\n",
    "def should_review(state: TaxonomyGenerationState) -> str:\n",
    "    num_minibatches = len(state['minibatches'])\n",
    "    num_revisions = len(state['clusters'])\n",
    "    if num_revisions < num_minibatches:\n",
    "        return \"update_taxonomy\"\n",
    "    return \"review_taxonomy\"\n",
    "graph.add_conditional_edges(\n",
    "    \"update_taxonomy\",\n",
    "    should_review,\n",
    "    # Optional (but required for the diagram to be drawn correctly below)\n",
    "    {\"update_taxonomy\": \"update_taxonomy\", 'review_taxonomy': \"review_taxonomy\"},\n",
    ")\n",
    "graph.add_edge('review_taxonomy', END)\n",
    "graph.add_edge(START, 'summarize')\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(app.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithNotFoundError",
     "evalue": "Project YOUR PROJECT NAME not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLangSmithNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m Client()\n\u001b[1;32m      8\u001b[0m past_week \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m runs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_runs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meq(is_root, true)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_week\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# We only need to return the inputs + outputs\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mselect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert the langsmith traces to our graph's Doc object.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_to_doc\u001b[39m(run) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langsmith/client.py:1714\u001b[0m, in \u001b[0;36mClient.list_runs\u001b[0;34m(self, project_id, project_name, run_type, trace_id, reference_example_id, query, filter, trace_filter, tree_filter, is_root, parent_run_id, start_time, error, run_ids, select, limit, **kwargs)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(project_name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1712\u001b[0m         project_name \u001b[38;5;241m=\u001b[39m [project_name]\n\u001b[1;32m   1713\u001b[0m     project_ids\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m-> 1714\u001b[0m         \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_project\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1715\u001b[0m     )\n\u001b[1;32m   1716\u001b[0m default_select \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapp_path\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchild_run_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1744\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1745\u001b[0m ]\n\u001b[1;32m   1746\u001b[0m select \u001b[38;5;241m=\u001b[39m select \u001b[38;5;129;01mor\u001b[39;00m default_select\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langsmith/client.py:1714\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(project_name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1712\u001b[0m         project_name \u001b[38;5;241m=\u001b[39m [project_name]\n\u001b[1;32m   1713\u001b[0m     project_ids\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m-> 1714\u001b[0m         [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_project\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mid \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m project_name]\n\u001b[1;32m   1715\u001b[0m     )\n\u001b[1;32m   1716\u001b[0m default_select \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapp_path\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchild_run_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1744\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1745\u001b[0m ]\n\u001b[1;32m   1746\u001b[0m select \u001b[38;5;241m=\u001b[39m select \u001b[38;5;129;01mor\u001b[39;00m default_select\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langsmith/utils.py:119\u001b[0m, in \u001b[0;36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     invalid_group_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg_groups[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m invalid_groups]\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one argument in each of the following\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m groups must be defined:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langsmith/client.py:2301\u001b[0m, in \u001b[0;36mClient.read_project\u001b[0;34m(self, project_id, project_name, include_stats)\u001b[0m\n\u001b[1;32m   2299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   2300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithNotFoundError(\n\u001b[1;32m   2302\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProject \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2303\u001b[0m         )\n\u001b[1;32m   2304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mTracerSessionResult(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresult[\u001b[38;5;241m0\u001b[39m], _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url)\n\u001b[1;32m   2305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mTracerSessionResult(\n\u001b[1;32m   2306\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson(), _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url\n\u001b[1;32m   2307\u001b[0m )\n",
      "\u001b[0;31mLangSmithNotFoundError\u001b[0m: Project YOUR PROJECT NAME not found"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "project_name = \"YOUR PROJECT NAME\"  # Update to your own project\n",
    "client = Client()\n",
    "\n",
    "past_week = datetime.now() - timedelta(days=7)\n",
    "runs = list(\n",
    "    client.list_runs(\n",
    "        project_name=project_name,\n",
    "        filter=\"eq(is_root, true)\",\n",
    "        start_time=past_week,\n",
    "        # We only need to return the inputs + outputs\n",
    "        select=[\"inputs\", \"outputs\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the langsmith traces to our graph's Doc object.\n",
    "def run_to_doc(run) -> Doc:\n",
    "    turns = []\n",
    "    idx = 0\n",
    "    for turn in run.inputs.get(\"chat_history\") or []:\n",
    "        key, value = next(iter(turn.items()))\n",
    "        turns.append(f\"<{key} idx={idx}>\\n{value}\\n\")\n",
    "        idx += 1\n",
    "    turns.append(\n",
    "        f\"\"\"\n",
    "\n",
    "{run.inputs['question']}\n",
    "\"\"\"\n",
    "    )\n",
    "    if run.outputs and run.outputs[\"output\"]:\n",
    "        turns.append(\n",
    "            f\"\"\"\n",
    "{run.outputs['output']}\n",
    "\"\"\"\n",
    "        )\n",
    "    return {\n",
    "        \"id\": str(run.id),\n",
    "        \"content\": (\"\\n\".join(turns)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
